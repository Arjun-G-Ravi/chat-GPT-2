# Welcome to chat GPT2
This is the code with which I fine tuned OpenAI's GPT2(which is made for text completion) to a chatting model. The model seems to be very good for a 124M parameter model in general knowledge. Intended purpose of the model: To create a powerful, easy to use and reliable model to be run on a consumer level graphics card (or maybe even a CPU). This model vastly outperforms GPT2 and many other similar parameter models.

### To use the model. click this huggingface link: 


### Model Description

```
license: mit
dataset: MuskumPillerum/General-Knowledge
base_model: gpt2
Finetuned epochs: 50
Finetune loss: 0.06
```